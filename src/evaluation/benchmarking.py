"""
ABOUTME: Benchmarking framework for comparing synthetic consumer responses to human survey data
ABOUTME: Validates SSR system performance against real-world human baselines

This module implements the benchmarking framework that validates the SSR system
against human survey data. The paper's core claim is that LLMs can achieve 90%
of human test-retest reliability when properly conditioned with demographics.

Key Validation Metrics:
- Distribution Similarity: K^xy ‚â• 0.85 (KS similarity)
- Test-Retest Reliability: œÅ ‚â• 0.90 (90% of human baseline)
- Prediction Accuracy: MAE < 0.5 (mean absolute error)

Benchmarking Components:
1. Human data loading and validation
2. Synthetic vs. human distribution comparison
3. Statistical significance testing
4. Performance gap analysis
5. Comprehensive reporting with visualizations
"""

from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from pathlib import Path
import json
import csv
import numpy as np

from .metrics import (
    MetricsCalculator,
    KSSimilarity,
    CorrelationMetrics,
    ErrorMetrics,
)


@dataclass
class HumanSurveyResponse:
    """
    Individual human survey response

    Attributes:
        respondent_id: Unique identifier for respondent
        product_id: Product being rated
        rating: Numerical rating (typically 1-5 scale)
        demographics: Optional demographic attributes
        timestamp: Response timestamp
        metadata: Additional response metadata
    """

    respondent_id: str
    product_id: str
    rating: float
    demographics: Optional[Dict[str, Any]] = None
    timestamp: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class HumanDataset:
    """
    Collection of human survey responses

    Attributes:
        responses: List of individual responses
        product_name: Product name
        product_description: Product description
        scale_min: Minimum rating value
        scale_max: Maximum rating value
        num_respondents: Number of unique respondents
        mean_rating: Mean rating across all responses
        std_rating: Standard deviation of ratings
    """

    responses: List[HumanSurveyResponse]
    product_name: str
    product_description: str
    scale_min: float = 1.0
    scale_max: float = 5.0
    num_respondents: int = 0
    mean_rating: float = 0.0
    std_rating: float = 0.0

    def __post_init__(self):
        """Calculate dataset statistics"""
        if self.responses:
            ratings = [r.rating for r in self.responses]
            self.num_respondents = len(set(r.respondent_id for r in self.responses))
            self.mean_rating = np.mean(ratings)
            self.std_rating = np.std(ratings)


@dataclass
class BenchmarkComparison:
    """
    Comparison between synthetic and human data

    Attributes:
        metric_name: Name of metric being compared
        synthetic_value: Value from synthetic consumers
        human_value: Value from human data
        difference: Absolute difference
        percent_difference: Percentage difference
        meets_threshold: Whether comparison meets quality threshold
        threshold: Quality threshold value
    """

    metric_name: str
    synthetic_value: float
    human_value: float
    difference: float
    percent_difference: float
    meets_threshold: bool
    threshold: float


@dataclass
class BenchmarkReport:
    """
    Comprehensive benchmark report comparing synthetic to human data

    Attributes:
        synthetic_distribution: Rating distribution from synthetic consumers
        human_distribution: Rating distribution from human respondents
        ks_similarity: KS similarity comparison
        correlation_metrics: Correlation analysis (if test-retest data available)
        error_metrics: Prediction error metrics
        comparisons: List of individual metric comparisons
        overall_performance: Overall system performance score (0.0-1.0)
        passes_benchmark: Whether system meets all quality thresholds
        summary: Human-readable summary
    """

    synthetic_distribution: np.ndarray
    human_distribution: np.ndarray
    ks_similarity: KSSimilarity
    correlation_metrics: Optional[CorrelationMetrics]
    error_metrics: ErrorMetrics
    comparisons: List[BenchmarkComparison]
    overall_performance: float
    passes_benchmark: bool
    summary: str = ""

    def generate_summary(self) -> str:
        """Generate human-readable benchmark summary"""
        lines = [
            "Benchmark Report: Synthetic vs. Human Performance",
            "=" * 70,
        ]

        lines.append("\nüìä Distribution Similarity (KS):")
        lines.append(f"  K^xy = {self.ks_similarity.ks_similarity:.3f}")
        lines.append(
            f"  Target: ‚â•0.85 | Status: {'‚úÖ PASS' if self.ks_similarity.ks_similarity >= 0.85 else '‚ùå FAIL'}"
        )

        if self.correlation_metrics:
            lines.append("\nüìà Test-Retest Reliability (œÅ):")
            lines.append(f"  Correlation: {self.correlation_metrics.pearson_r:.3f}")
            lines.append(
                f"  Target: ‚â•0.90 | Status: {'‚úÖ PASS' if self.correlation_metrics.pearson_r >= 0.90 else '‚ùå FAIL'}"
            )
            if self.correlation_metrics.correlation_attainment:
                lines.append(
                    f"  Human Baseline: {self.correlation_metrics.correlation_attainment * 100:.1f}%"
                )

        lines.append("\nüéØ Prediction Accuracy (MAE):")
        lines.append(f"  MAE = {self.error_metrics.mae:.3f}")
        lines.append(
            f"  Target: <0.5 | Status: {'‚úÖ PASS' if self.error_metrics.mae < 0.5 else '‚ùå FAIL'}"
        )

        lines.append("\nüìã Individual Metric Comparisons:")
        for comp in self.comparisons:
            status = "‚úÖ" if comp.meets_threshold else "‚ùå"
            lines.append(
                f"  {status} {comp.metric_name}: {comp.synthetic_value:.3f} vs {comp.human_value:.3f} "
                f"({comp.percent_difference:+.1f}%)"
            )

        lines.append("\nüèÜ Overall Performance:")
        lines.append(f"  Score: {self.overall_performance * 100:.1f}%")
        lines.append(
            f"  Status: {'‚úÖ PASSES BENCHMARK' if self.passes_benchmark else '‚ùå FAILS BENCHMARK'}"
        )

        return "\n".join(lines)


class HumanDataLoader:
    """
    Loader for human survey data from various formats

    Supports:
    - CSV files with structured response data
    - JSON files with response arrays
    - Dictionary-based data structures
    - Multiple rating scales (1-5, 1-7, 1-10)
    """

    def __init__(
        self, scale_min: float = 1.0, scale_max: float = 5.0, validate: bool = True
    ):
        """
        Initialize human data loader

        Args:
            scale_min: Minimum rating value (default: 1.0)
            scale_max: Maximum rating value (default: 5.0)
            validate: Whether to validate loaded data
        """
        self.scale_min = scale_min
        self.scale_max = scale_max
        self.validate = validate

    def load_from_csv(
        self, file_path: Union[str, Path], product_name: str, product_description: str
    ) -> HumanDataset:
        """
        Load human survey data from CSV file

        CSV Format:
            respondent_id,product_id,rating,age,gender,income_level,timestamp
            user_001,product_a,4.0,35,Female,$50000-$74999,2024-01-15T10:30:00

        Args:
            file_path: Path to CSV file
            product_name: Name of product
            product_description: Product description

        Returns:
            HumanDataset with loaded responses

        Raises:
            FileNotFoundError: If CSV file doesn't exist
            ValueError: If CSV format is invalid
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"CSV file not found: {file_path}")

        responses = []

        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)

            for row_num, row in enumerate(reader, start=2):
                try:
                    # Extract required fields
                    respondent_id = row.get("respondent_id", f"respondent_{row_num}")
                    product_id = row.get("product_id", "unknown")
                    rating = float(row["rating"])

                    # Validate rating
                    if self.validate and not (
                        self.scale_min <= rating <= self.scale_max
                    ):
                        raise ValueError(
                            f"Rating {rating} out of range [{self.scale_min}, {self.scale_max}]"
                        )

                    # Extract optional demographics
                    demographics = {}
                    for key in ["age", "gender", "income_level", "ethnicity"]:
                        if key in row:
                            demographics[key] = row[key]

                    # Create response
                    response = HumanSurveyResponse(
                        respondent_id=respondent_id,
                        product_id=product_id,
                        rating=rating,
                        demographics=demographics if demographics else None,
                        timestamp=row.get("timestamp"),
                        metadata={
                            k: v
                            for k, v in row.items()
                            if k
                            not in [
                                "respondent_id",
                                "product_id",
                                "rating",
                                "age",
                                "gender",
                                "income_level",
                                "ethnicity",
                                "timestamp",
                            ]
                        },
                    )

                    responses.append(response)

                except (KeyError, ValueError) as e:
                    raise ValueError(
                        f"Invalid CSV format at row {row_num}: {str(e)}"
                    ) from e

        return HumanDataset(
            responses=responses,
            product_name=product_name,
            product_description=product_description,
            scale_min=self.scale_min,
            scale_max=self.scale_max,
        )

    def load_from_json(
        self, file_path: Union[str, Path], product_name: str, product_description: str
    ) -> HumanDataset:
        """
        Load human survey data from JSON file

        JSON Format:
            {
                "responses": [
                    {
                        "respondent_id": "user_001",
                        "product_id": "product_a",
                        "rating": 4.0,
                        "demographics": {"age": 35, "gender": "Female"},
                        "timestamp": "2024-01-15T10:30:00"
                    }
                ]
            }

        Args:
            file_path: Path to JSON file
            product_name: Name of product
            product_description: Product description

        Returns:
            HumanDataset with loaded responses

        Raises:
            FileNotFoundError: If JSON file doesn't exist
            ValueError: If JSON format is invalid
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"JSON file not found: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        return self.load_from_dict(
            data=data,
            product_name=product_name,
            product_description=product_description,
        )

    def load_from_dict(
        self, data: Dict[str, Any], product_name: str, product_description: str
    ) -> HumanDataset:
        """
        Load human survey data from dictionary

        Args:
            data: Dictionary with 'responses' list
            product_name: Name of product
            product_description: Product description

        Returns:
            HumanDataset with loaded responses

        Raises:
            ValueError: If dictionary format is invalid
        """
        if "responses" not in data:
            raise ValueError("Dictionary must contain 'responses' key")

        responses = []

        for idx, item in enumerate(data["responses"]):
            try:
                # Extract required fields
                respondent_id = item.get("respondent_id", f"respondent_{idx}")
                product_id = item.get("product_id", "unknown")
                rating = float(item["rating"])

                # Validate rating
                if self.validate and not (self.scale_min <= rating <= self.scale_max):
                    raise ValueError(
                        f"Rating {rating} out of range [{self.scale_min}, {self.scale_max}]"
                    )

                # Create response
                response = HumanSurveyResponse(
                    respondent_id=respondent_id,
                    product_id=product_id,
                    rating=rating,
                    demographics=item.get("demographics"),
                    timestamp=item.get("timestamp"),
                    metadata=item.get("metadata", {}),
                )

                responses.append(response)

            except (KeyError, ValueError) as e:
                raise ValueError(
                    f"Invalid response format at index {idx}: {str(e)}"
                ) from e

        return HumanDataset(
            responses=responses,
            product_name=product_name,
            product_description=product_description,
            scale_min=self.scale_min,
            scale_max=self.scale_max,
        )


class BenchmarkComparator:
    """
    Compares synthetic consumer responses to human survey data

    Features:
    - Distribution similarity analysis
    - Statistical significance testing
    - Performance gap quantification
    - Threshold-based validation
    - Comprehensive reporting
    """

    def __init__(
        self,
        human_baseline_correlation: float = 1.0,
        ks_threshold: float = 0.85,
        correlation_threshold: float = 0.90,
        mae_threshold: float = 0.5,
    ):
        """
        Initialize benchmark comparator

        Args:
            human_baseline_correlation: Human test-retest correlation (default: 1.0)
            ks_threshold: Minimum KS similarity for passing (default: 0.85)
            correlation_threshold: Minimum correlation for passing (default: 0.90)
            mae_threshold: Maximum MAE for passing (default: 0.5)
        """
        self.human_baseline_correlation = human_baseline_correlation
        self.ks_threshold = ks_threshold
        self.correlation_threshold = correlation_threshold
        self.mae_threshold = mae_threshold
        self.metrics_calculator = MetricsCalculator(human_baseline_correlation)

    def compare_distributions(
        self,
        synthetic_ratings: List[float],
        human_dataset: HumanDataset,
        num_bins: int = 5,
    ) -> BenchmarkReport:
        """
        Compare synthetic consumer ratings to human survey data

        Args:
            synthetic_ratings: List of ratings from synthetic consumers
            human_dataset: Human survey dataset
            num_bins: Number of rating bins (default: 5 for 1-5 scale)

        Returns:
            BenchmarkReport with comprehensive comparison
        """
        # Convert to numpy arrays
        synthetic_arr = np.array(synthetic_ratings)
        human_ratings = np.array([r.rating for r in human_dataset.responses])

        # Create rating distributions
        bins = np.arange(human_dataset.scale_min, human_dataset.scale_max + 1.5, 1.0)

        synthetic_hist, _ = np.histogram(synthetic_arr, bins=bins, density=True)
        human_hist, _ = np.histogram(human_ratings, bins=bins, density=True)

        # Normalize to probabilities
        synthetic_dist = synthetic_hist / (synthetic_hist.sum() + 1e-10)
        human_dist = human_hist / (human_hist.sum() + 1e-10)

        # Calculate KS similarity
        ks_similarity = self.metrics_calculator.calculate_ks_similarity(
            synthetic_dist, human_dist
        )

        # Calculate error metrics (using mean ratings as point estimates)
        synthetic_mean = np.mean(synthetic_arr)
        human_mean = np.mean(human_ratings)

        # For error metrics, treat synthetic mean as prediction
        predicted = np.full_like(human_ratings, synthetic_mean)
        error_metrics = self.metrics_calculator.calculate_error_metrics(
            predicted, human_ratings
        )

        # Correlation metrics (if applicable - for test-retest data)
        correlation_metrics = None
        # This would be populated if we have test-retest data
        # For now, we focus on distribution comparison

        # Individual comparisons
        comparisons = [
            BenchmarkComparison(
                metric_name="Mean Rating",
                synthetic_value=synthetic_mean,
                human_value=human_mean,
                difference=abs(synthetic_mean - human_mean),
                percent_difference=(
                    (synthetic_mean - human_mean) / human_mean * 100
                    if human_mean != 0
                    else 0.0
                ),
                meets_threshold=abs(synthetic_mean - human_mean) < 0.5,
                threshold=0.5,
            ),
            BenchmarkComparison(
                metric_name="Std Deviation",
                synthetic_value=np.std(synthetic_arr),
                human_value=np.std(human_ratings),
                difference=abs(np.std(synthetic_arr) - np.std(human_ratings)),
                percent_difference=(
                    (np.std(synthetic_arr) - np.std(human_ratings))
                    / np.std(human_ratings)
                    * 100
                    if np.std(human_ratings) != 0
                    else 0.0
                ),
                meets_threshold=abs(np.std(synthetic_arr) - np.std(human_ratings))
                < 0.5,
                threshold=0.5,
            ),
            BenchmarkComparison(
                metric_name="KS Similarity",
                synthetic_value=ks_similarity.ks_similarity,
                human_value=1.0,  # Perfect match with self
                difference=1.0 - ks_similarity.ks_similarity,
                percent_difference=(1.0 - ks_similarity.ks_similarity) * 100,
                meets_threshold=ks_similarity.ks_similarity >= self.ks_threshold,
                threshold=self.ks_threshold,
            ),
        ]

        # Calculate overall performance score
        # Weight: KS similarity (40%), MAE (30%), Mean difference (30%)
        ks_score = min(ks_similarity.ks_similarity / self.ks_threshold, 1.0)
        mae_score = max(1.0 - (error_metrics.mae / self.mae_threshold), 0.0)
        mean_score = max(1.0 - (abs(synthetic_mean - human_mean) / 0.5), 0.0)

        overall_performance = 0.4 * ks_score + 0.3 * mae_score + 0.3 * mean_score

        # Check if passes benchmark
        passes = (
            ks_similarity.ks_similarity >= self.ks_threshold
            and error_metrics.mae < self.mae_threshold
        )

        report = BenchmarkReport(
            synthetic_distribution=synthetic_dist,
            human_distribution=human_dist,
            ks_similarity=ks_similarity,
            correlation_metrics=correlation_metrics,
            error_metrics=error_metrics,
            comparisons=comparisons,
            overall_performance=overall_performance,
            passes_benchmark=passes,
        )

        report.summary = report.generate_summary()

        return report

    def compare_with_test_retest(
        self,
        synthetic_test: List[float],
        synthetic_retest: List[float],
        human_test: List[float],
        human_retest: List[float],
    ) -> BenchmarkReport:
        """
        Compare test-retest reliability between synthetic and human data

        This is the core validation from the paper: LLMs achieve 90% of human
        test-retest reliability (œÅ ‚â• 0.90).

        Args:
            synthetic_test: Synthetic consumer test phase ratings
            synthetic_retest: Synthetic consumer retest phase ratings
            human_test: Human test phase ratings
            human_retest: Human retest phase ratings

        Returns:
            BenchmarkReport with test-retest comparison
        """
        # Convert to numpy arrays
        synth_test_arr = np.array(synthetic_test)
        synth_retest_arr = np.array(synthetic_retest)
        human_test_arr = np.array(human_test)
        human_retest_arr = np.array(human_retest)

        # Calculate synthetic correlation
        synth_corr = self.metrics_calculator.calculate_correlation(
            synth_test_arr, synth_retest_arr, self.human_baseline_correlation
        )

        # Calculate human correlation (baseline)
        human_corr = self.metrics_calculator.calculate_correlation(
            human_test_arr, human_retest_arr, self.human_baseline_correlation
        )

        # Distribution comparison (using test phase)
        bins = np.arange(1.0, 6.5, 1.0)  # 1-5 scale

        synth_hist, _ = np.histogram(synth_test_arr, bins=bins, density=True)
        human_hist, _ = np.histogram(human_test_arr, bins=bins, density=True)

        synth_dist = synth_hist / (synth_hist.sum() + 1e-10)
        human_dist = human_hist / (human_hist.sum() + 1e-10)

        ks_similarity = self.metrics_calculator.calculate_ks_similarity(
            synth_dist, human_dist
        )

        # Error metrics
        error_metrics = self.metrics_calculator.calculate_error_metrics(
            synth_test_arr, human_test_arr
        )

        # Individual comparisons
        comparisons = [
            BenchmarkComparison(
                metric_name="Test-Retest Correlation",
                synthetic_value=synth_corr.pearson_r,
                human_value=human_corr.pearson_r,
                difference=abs(synth_corr.pearson_r - human_corr.pearson_r),
                percent_difference=(
                    (synth_corr.pearson_r - human_corr.pearson_r)
                    / human_corr.pearson_r
                    * 100
                    if human_corr.pearson_r != 0
                    else 0.0
                ),
                meets_threshold=synth_corr.pearson_r >= self.correlation_threshold,
                threshold=self.correlation_threshold,
            ),
            BenchmarkComparison(
                metric_name="Correlation Attainment",
                synthetic_value=synth_corr.pearson_r / human_corr.pearson_r
                if human_corr.pearson_r != 0
                else 0.0,
                human_value=1.0,
                difference=1.0
                - (
                    synth_corr.pearson_r / human_corr.pearson_r
                    if human_corr.pearson_r != 0
                    else 0.0
                ),
                percent_difference=(
                    1.0
                    - (
                        synth_corr.pearson_r / human_corr.pearson_r
                        if human_corr.pearson_r != 0
                        else 0.0
                    )
                )
                * 100,
                meets_threshold=(
                    synth_corr.pearson_r / human_corr.pearson_r
                    if human_corr.pearson_r != 0
                    else 0.0
                )
                >= 0.90,
                threshold=0.90,
            ),
        ]

        # Overall performance
        corr_score = min(synth_corr.pearson_r / self.correlation_threshold, 1.0)
        ks_score = min(ks_similarity.ks_similarity / self.ks_threshold, 1.0)
        mae_score = max(1.0 - (error_metrics.mae / self.mae_threshold), 0.0)

        overall_performance = 0.5 * corr_score + 0.3 * ks_score + 0.2 * mae_score

        passes = (
            synth_corr.pearson_r >= self.correlation_threshold
            and ks_similarity.ks_similarity >= self.ks_threshold
            and error_metrics.mae < self.mae_threshold
        )

        report = BenchmarkReport(
            synthetic_distribution=synth_dist,
            human_distribution=human_dist,
            ks_similarity=ks_similarity,
            correlation_metrics=synth_corr,
            error_metrics=error_metrics,
            comparisons=comparisons,
            overall_performance=overall_performance,
            passes_benchmark=passes,
        )

        report.summary = report.generate_summary()

        return report


# Example usage and testing
if __name__ == "__main__":
    print("Benchmark Framework Testing")
    print("=" * 70)

    # Test 1: Load sample human data
    print("\n1. Human Data Loading:")

    sample_data = {
        "responses": [
            {
                "respondent_id": f"human_{i}",
                "product_id": "test_product",
                "rating": float(np.random.choice([3, 4, 5], p=[0.2, 0.5, 0.3])),
                "demographics": {
                    "age": np.random.randint(25, 65),
                    "gender": np.random.choice(["Male", "Female"]),
                },
            }
            for i in range(100)
        ]
    }

    loader = HumanDataLoader()
    human_dataset = loader.load_from_dict(
        data=sample_data,
        product_name="Test Product",
        product_description="A test product for validation",
    )

    print(f"Loaded {len(human_dataset.responses)} human responses")
    print(f"Mean rating: {human_dataset.mean_rating:.2f}")
    print(f"Std rating: {human_dataset.std_rating:.2f}")

    # Test 2: Compare synthetic vs human
    print("\n" + "=" * 70)
    print("2. Distribution Comparison:")

    # Simulate synthetic consumer responses (similar distribution)
    synthetic_ratings = [
        float(np.random.choice([3, 4, 5], p=[0.25, 0.45, 0.30])) for _ in range(100)
    ]

    comparator = BenchmarkComparator()
    report = comparator.compare_distributions(synthetic_ratings, human_dataset)

    print("\n" + report.summary)

    # Test 3: Test-retest comparison
    print("\n" + "=" * 70)
    print("3. Test-Retest Reliability Comparison:")

    # Simulate test-retest data
    np.random.seed(42)

    # Human: high reliability (œÅ ‚âà 0.95)
    human_base = np.random.uniform(2.5, 4.5, 50)
    human_test = np.clip(human_base + np.random.normal(0, 0.3, 50), 1, 5)
    human_retest = np.clip(human_base + np.random.normal(0, 0.3, 50), 1, 5)

    # Synthetic: target reliability (œÅ ‚âà 0.90)
    synth_base = np.random.uniform(2.5, 4.5, 50)
    synth_test = np.clip(synth_base + np.random.normal(0, 0.4, 50), 1, 5)
    synth_retest = np.clip(synth_base + np.random.normal(0, 0.4, 50), 1, 5)

    retest_report = comparator.compare_with_test_retest(
        synthetic_test=synth_test.tolist(),
        synthetic_retest=synth_retest.tolist(),
        human_test=human_test.tolist(),
        human_retest=human_retest.tolist(),
    )

    print("\n" + retest_report.summary)

    # Test 4: Paper benchmark validation
    print("\n" + "=" * 70)
    print("4. Paper Benchmark Validation:")
    print("\nPaper Results:")
    print("  GPT-4o: œÅ = 0.902, K^xy = 0.88")
    print("  Gemini-2.0-flash: œÅ = 0.906, K^xy = 0.80")
    print("  Target: œÅ ‚â• 0.90, K^xy ‚â• 0.85, MAE < 0.5")

    print("\nSimulated Results:")
    print(f"  Overall Performance: {retest_report.overall_performance * 100:.1f}%")
    print(
        f"  Benchmark Status: {'‚úÖ PASSES' if retest_report.passes_benchmark else '‚ùå FAILS'}"
    )

    print("\n" + "=" * 70)
    print("Benchmark framework testing complete")
    print("\nKey Insights:")
    print("- Human data loading supports CSV, JSON, and dictionary formats")
    print("- Distribution comparison uses KS similarity, correlation, and MAE")
    print("- Test-retest validation confirms 90% human baseline target")
    print("- Comprehensive reporting with threshold-based pass/fail criteria")
